{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewgcodes/lightspeedEmbeddings/blob/main/demoLightspeedEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "SYjsy-9XVGOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "Y4xWy_rwFwmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7156f18-0654-49fd-a190-520d44c4ba76"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.8 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multithreading"
      ],
      "metadata": {
        "id": "-tG4QpbIVIXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EucrG-OSFfzA",
        "outputId": "b9cf7615-f7e7-4472-aca4-1292a7bb6192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding texts: 100%|██████████| 1000/1000 [00:10<00:00, 91.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.78279447555542\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "import openai\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import concurrent.futures\n",
        "import time\n",
        "from openai.embeddings_utils import get_embedding\n",
        "from requests.exceptions import HTTPError\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "# set your OpenAI API key here.\n",
        "openai.api_key=\"sk-INSERTAPIKEY\"\n",
        "\n",
        "# embedding model parameters\n",
        "\n",
        "# you can change the embeddings model but ada-002 is the best in quality and cost\n",
        "embedding_model = \"text-embedding-ada-002\"\n",
        "\n",
        "# probably don't touch this\n",
        "embedding_encoding = \"cl100k_base\"\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# code is set to ignore documents/texts that are too long (more than about 6,000 words)\n",
        "# there is a limit of ~8,000 tokens to be embedded by the OpenAI models\n",
        "# you should not exceed 8,000 here\n",
        "max_tokens = 4000\n",
        "\n",
        "def load_data(df, method='filter'):\n",
        "    encoding = tiktoken.get_encoding(embedding_encoding)\n",
        "    df[\"n_tokens\"] = df.combined.apply(lambda x: len(encoding.encode(x)))\n",
        "\n",
        "    if method == 'filter':\n",
        "        df = df[df.n_tokens <= max_tokens]\n",
        "    elif method == 'truncate':\n",
        "        def truncate_to_max_tokens(text):\n",
        "            tokens = encoding.encode(text)\n",
        "            return encoding.decode(tokens[:max_tokens])\n",
        "\n",
        "        df['combined'] = df['combined'].apply(truncate_to_max_tokens)\n",
        "    else:\n",
        "        raise ValueError(f'Invalid method {method}. Choose \"filter\" or \"truncate\".')\n",
        "\n",
        "    return df\n",
        "\n",
        "def get_embeddings(text, max_retries=5):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            return get_embedding(text, engine=embedding_model)\n",
        "        except HTTPError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                print('Rate limit exceeded. Sleeping for a while before retrying...')\n",
        "                time.sleep(30)\n",
        "                retries += 1\n",
        "            else:\n",
        "                raise e\n",
        "    raise Exception('Failed to get embedding after multiple retries')\n",
        "\n",
        "def get_embeddings_parallel(df, n_threads=10, logfile='embeddings_errors.log'):\n",
        "    logging.basicConfig(filename=logfile, level=logging.ERROR)\n",
        "    logger = logging.getLogger()\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
        "        future_to_text = {executor.submit(get_embeddings, text): text for text in df['combined']}\n",
        "        for future in tqdm(concurrent.futures.as_completed(future_to_text), total=len(future_to_text), desc='Embedding texts'):\n",
        "            text = future_to_text[future]\n",
        "            try:\n",
        "                df.loc[df['combined'] == text, 'embedding'] = str(future.result())\n",
        "            except Exception as e:\n",
        "                error_message = f'Failed to get embedding for text: {text}. Exception: {e}'\n",
        "                print(error_message)\n",
        "                logger.error(error_message)  # Log the error message\n",
        "    return df\n",
        "\n",
        "def process_data(input_datapath, output_datapath, logfile, method='filter'):\n",
        "    df = load_data(input_datapath, method)\n",
        "    df = get_embeddings_parallel(df, logfile=logfile)\n",
        "    df.to_csv(output_datapath)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_datapath = \"fine_food_reviews_1k.csv\" #REPLACE WITH YOUR CSV FILE\n",
        "    output_file_name = \"fine_food_reviews_with_embeddings_1k.csv\" #REPLACE WITH DESIRED OUTPUT FILE NAME\n",
        "    log_file_name = \"embedding_errors.log\"  # Log file name\n",
        "    # READ IN YOUR CSV\n",
        "    df = pd.read_csv(input_datapath, index_col=0)\n",
        "\n",
        "    # This is code SPECIFIC to the Fine Food Reviews dataset!\n",
        "    # You will need to modify it to fit your own dataset.\n",
        "\n",
        "    df = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n",
        "    df = df.dropna()\n",
        "\n",
        "    # LightspeedEmbeddings is just looking for a dataframe with a column called combined.\n",
        "    # It will calculate the embeddings for each cell in the combined column.\n",
        "    df[\"combined\"] = (\n",
        "        \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n",
        "    )\n",
        "    df.drop(\"Time\", axis=1, inplace=True)\n",
        "\n",
        "    # timing code\n",
        "    start_time = time.time()\n",
        "\n",
        "    # this actually starts getting the embeddings\n",
        "    # you can select method = \"filter\" which removes texts that are longer than max_tokens\n",
        "    # or you can select method = \"truncate\" which will chop away at too-long texts until they are smaller than max_tokens\n",
        "    process_data(df, output_file_name, log_file_name, method = \"truncate\")\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # see how long it took!\n",
        "    print(end_time-start_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sanity check the results here. Try different queries."
      ],
      "metadata": {
        "id": "mVM2ztfjTnbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We store the embedding vector as a string in one 'cell' in the dataframe.\n",
        "# You need to use .apply(eval).apply(np.array) to convert the strings back into vectors\n",
        "# This step takes a while.\n",
        "df = pd.read_csv('fine_food_reviews_with_embeddings_1k.csv')\n",
        "df[\"embedding\"] = df.embedding.apply(eval).apply(np.array)"
      ],
      "metadata": {
        "id": "_ivYRHDJRN8M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here is code directly taken from the OpenAI Cookbook tutorial on semantic search.\n",
        "\n",
        "[Link](https://github.com/openai/openai-cookbook/blob/main/examples/Semantic_text_search_using_embeddings.ipynb)"
      ],
      "metadata": {
        "id": "noBNlYV9U47C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "\n",
        "# search through the reviews for a specific product\n",
        "def search_reviews(df, product_description, n=3, pprint=True):\n",
        "    product_embedding = get_embedding(\n",
        "        product_description,\n",
        "        engine=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    df[\"similarity\"] = df.embedding.apply(lambda x: cosine_similarity(x, product_embedding))\n",
        "\n",
        "    results = (\n",
        "        df.sort_values(\"similarity\", ascending=False)\n",
        "        .head(n)\n",
        "        .combined.str.replace(\"Title: \", \"\")\n",
        "        .str.replace(\"; Content:\", \": \")\n",
        "    )\n",
        "    if pprint:\n",
        "        for r in results:\n",
        "            print(r[:200])\n",
        "            print()\n",
        "    return results\n",
        "\n",
        "\n",
        "results = search_reviews(df, \"ramen\", n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D3VnxztRjuN",
        "outputId": "4c08eb84-1bb3-4340-9de8-dd61c5806e91"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "warning! cancer...:  I love this stuff, but sadly just received a phone call from my mother who was watching Taiwanese news -- they recently were warning consumers in Asia NOT to buy Shin Ramyun (this\n",
            "\n",
            "Instant noodle with best taste and texture -- see Recall Info:  I just researched about the korean noodle recall because I love this Shin Ramyun noodle -- the best taste and texture instant noodle I h\n",
            "\n",
            "Best cup of noodles ever!:  Tried many (from all over the world) different types of packaged quick-meal type noodles.  These are by far the best I've had.\n",
            "\n",
            "Great stuff:  I use this to make a broth for noodles and soup. it reminds me of the days I spent in Japan. easy to use.\n",
            "\n",
            "Fine for a microwave dinner:  The Barilla Mezze Penne with spicy marinara sauce is easy to prepare and tastes better than similar products. The sauce is not as spicy as I expected it to be but does ha\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You can download your embeddings CSV file from the folder panel on the lefthand side of Google Colab."
      ],
      "metadata": {
        "id": "R8VTiGWyTse4"
      }
    }
  ]
}